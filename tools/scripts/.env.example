# ===========================================
# AI Scripts Environment Configuration
# ===========================================
# Copy this file to .env and fill in your values

# LLM Provider: "gemini" or "ollama"
# gemini = Google Gemini API (requires API key)
# ollama = Local Ollama instance (free, private)
LLM_PROVIDER=gemini

# ===========================================
# Gemini Configuration
# ===========================================
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your-api-key-here

# Model to use (see: bun release:gen --list-models)
GEMINI_MODEL=gemini-2.0-flash

# ===========================================
# Ollama Configuration (Local AI)
# ===========================================
# Model name (run: ollama list to see available models)
OLLAMA_MODEL=llama3

# Ollama server host (use IP for remote, localhost for local)
OLLAMA_HOST=localhost

# Ollama server port (default: 11434)
OLLAMA_PORT=11434
